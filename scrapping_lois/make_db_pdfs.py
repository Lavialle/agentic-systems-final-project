import os
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time

BASE_URL = "https://www.assemblee-nationale.fr"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_DIR = os.path.join(BASE_DIR, "db") 
OUTPUT_DIR = os.path.join(DB_DIR, "pdf")
os.makedirs(OUTPUT_DIR, exist_ok=True)

unknown_counter = 1


def extract_id(url):
    patterns = {
        "proposition_loi": r"propositions/pion([\w-]+)\.asp",
        "projet_loi": r"projets/pl([\w-]+)\.asp",
        "rapport_legislatif": r"rapports/r([\w-]+)\.asp",
        "texte_adopte": r"/ta/ta([\w-]+)\.asp",
        "dossier_legislatif": r"/textes/l17b(\d+)_",
    }

    for dtype, pat in patterns.items():
        m = re.search(pat, url)
        if m:
            return dtype, m.group(1)

    return "inconnu", None


def download_pdf(doc_type, doc_id, pdf_url):
    """
    T√©l√©charge le PDF et retourne True si succ√®s (ou d√©j√† existant),
    False si √©chec.
    """
    global unknown_counter

    if not doc_id:
        filename = f"{doc_type}_unknown_{unknown_counter}.pdf"
        unknown_counter += 1
    else:
        filename = f"{doc_type}_{doc_id}.pdf"

    filepath = os.path.join(OUTPUT_DIR, filename)

    if os.path.exists(filepath):
        print(f"‚úîÔ∏è D√©j√† t√©l√©charg√© : {filename}")
        return True 

    print(f"‚¨áÔ∏è T√©l√©chargement : {filename}")

    try:
        r = requests.get(pdf_url, timeout=20)
        r.raise_for_status() 
        with open(filepath, "wb") as f:
            f.write(r.content)
        print(f"‚úÖ Fichier sauvegard√© : {filename}")
        return True 
    except Exception as e:
        print(f"‚ùå ERREUR de t√©l√©chargement pour {filename}: {e}")
        return False 


def get_pdf_link(page_url):
    try:
        r = requests.get(page_url, timeout=20)
        r.raise_for_status()
    except Exception as e:
        print(f"‚ùå Impossible de charger {page_url} ‚Üí {e}")
        return None

    soup = BeautifulSoup(r.text, "html.parser")
    a = soup.find("a", title="Acc√©der au document au format PDF")

    if not a:
        print("‚ö†Ô∏è Aucun PDF trouv√© sur cette page")
        return None

    pdf_rel_path = a.get("href")

    if not pdf_rel_path or not pdf_rel_path.endswith(".pdf"):
        print("‚ö†Ô∏è Lien trouv√©, mais ce n'est pas un PDF")
        return None

    return urljoin(BASE_URL, pdf_rel_path)

# -----------------------------------------------
# NOUVEAU: BOUCLE PRINCIPALE AVEC MISE √Ä JOUR DB
# -----------------------------------------------

df_path = os.path.join(DB_DIR, "db_urls.parquet")
print(f"Lecture de {df_path}...")
try:
    df = pd.read_parquet(df_path)
except FileNotFoundError:
    print(f"ERREUR: Le fichier {df_path} n'existe pas.")
    exit()

if "downloaded" not in df.columns:
    print("Cr√©ation de la colonne 'downloaded', initialis√©e √† False.")
    df["downloaded"] = False
else:
    print("Colonne 'downloaded' existante.")


print(f"\n=== D√âBUT DU TRAITEMENT: {len(df)} URLs √† v√©rifier ===")

for index, row in df.iterrows():
    url = row["url"]
    print(f"\n=== PAGE ({index + 1}/{len(df)}) : {url}")

    doc_type, doc_id = extract_id(url)
    print(f"‚Üí Type d√©tect√© : {doc_type}")
    print(f"‚Üí ID d√©tect√© : {doc_id if doc_id else 'AUCUN'}")

    pdf_url = get_pdf_link(url)
    if not pdf_url:
        print("‚ö†Ô∏è PDF introuvable ‚Üí on continue")
        df.at[index, "downloaded"] = False 
        continue


    success = download_pdf(doc_type, doc_id, pdf_url)


    df.at[index, "downloaded"] = success

    time.sleep(0.1)

print("\n=== FIN ‚Äî Tous les PDF trait√©s ===")
print(f"üíæ Sauvegarde du DataFrame mis √† jour dans {df_path}...")
df.write_parquet(df_path, index=False)
print("‚úÖ Base de donn√©es (db_urls.parquet) mise √† jour avec succ√®s.")