from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, ToolMessage, BaseMessage, HumanMessage
from typing import Annotated, Sequence, TypedDict
from langfuse import observe
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.runnables import RunnableConfig
from summarizer_agent import summarize_law_text
from tone_analysis_agent import analyze_tone_of_voice, create_law_title
from config import langfuse_handler, MAX_CHARS
from PyPDF2 import PdfReader

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# Initialiser le mod√®le
model = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1
)

# Tool : R√©sum√© des textes de loi
@tool
@observe(name="summarize_tool")
def summarize_tool(law_text: str):
    """Produit un r√©sum√© clair et compr√©hensible d'un texte de loi."""
    return summarize_law_text(law_text)

# Tool : Analyse du tone of voice
@tool
@observe(name="tone_analysis_tool")
def tone_analysis_tool(law_text: str):
    """Analyse le tone of voice des m√©dias √† propos d'un texte de loi."""
    law_title = create_law_title(law_text)
    return analyze_tone_of_voice(law_title)


tools=[summarize_tool, tone_analysis_tool]
tools_by_name = {tool.name: tool for tool in tools}
llm_model_with_tools = model.bind_tools(tools)

# Define our tool node
def tool_node(state: AgentState) -> AgentState:
    """
    Ex√©cute les outils s√©lectionn√©s par l'agent.
    
    Args:
        state: √âtat actuel du graphe contenant les messages
    
    Returns:
        Dict avec les messages de r√©sultat des outils
    """
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=tool_result,
                name=tool_call["name"],
                tool_call_id=tool_call["id"]
            )
        )
    return {"messages": outputs}

# Define the node that calls the llm model
def call_llm_node(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Appelle le mod√®le LLM avec les outils disponibles.
    
    Args:
        state: √âtat actuel du graphe
        config: Configuration incluant les callbacks Langfuse
    
    Returns:
        Dict avec la r√©ponse du mod√®le
    """
    response = llm_model_with_tools.invoke(state["messages"], config)
    return {"messages": [response]}

# Define the condition edge that determines whether to continue or not
def should_continue(state: AgentState) -> str:
    """
    D√©termine si l'agent doit continuer vers les outils ou terminer.
    
    Args:
        state: √âtat actuel du graphe
    
    Returns:
        "continue" si des outils doivent √™tre appel√©s, "end" sinon
    """
    last_message = state["messages"][-1]
    return "continue" if (hasattr(last_message, 'tool_calls') and last_message.tool_calls) else "end"

# Define a new graph
workflow = StateGraph(AgentState)

# Define the two nodes we will cycle between
workflow.add_node("agent", call_llm_node)
workflow.add_node("tool", tool_node)

# Set the entrypoint as `agent`
workflow.add_edge(START, "agent")

# We now add a conditional edge
workflow.add_conditional_edges(  
    "agent", 
    # Mapping: keys are strings, values are nodes.The output of `should_continue` is matched to a key, and the corresponding node is then called.
    should_continue,
    {
        # If `tools`, then call the tool node.
        "continue": "tool", 
        # Otherwise finish.
        "end": END,
        },
    )
# Aller directement √† END apr√®s l'ex√©cution des tools (pas de synth√®se)
workflow.add_edge("tool", END)
graph = workflow.compile()

# G√©n√©rer l'image PNG du graphe au d√©marrage
def generate_graph_png():
    """G√©n√®re une image PNG du graphe LangGraph."""
    try:
        graph_image = graph.get_graph().draw_mermaid_png()
        with open("agent_graph.png", "wb") as f:
            f.write(graph_image)
        print("‚úì Graphe g√©n√©r√© : agent_graph.png")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de g√©n√©rer le graphe PNG : {e}")
        return False

# G√©n√©rer le graphe au chargement du module
generate_graph_png()

SYSTEM_PROMPT_SIMPLE_AGENT = SystemMessage(
    content="""Assistant juridique. 2 outils disponibles :
- summarize_tool(law_text) : r√©sume la loi
- tone_analysis_tool(law_text) : analyse presse

R√àGLE STRICTE : Tu dois choisir UN SEUL outil √† la fois.
- Si r√©sum√© demand√© : utilise UNIQUEMENT summarize_tool
- Si analyse presse demand√©e : utilise UNIQUEMENT tone_analysis_tool
- Si "les deux" demand√© : choisis celui qui te semble le plus pertinent

Tu ne peux PAS appeler les deux outils simultan√©ment."""
)

# Fonction pour lire un fichier PDF
def read_pdf(file_source) -> str:
    """
    Lit un fichier PDF et extrait son contenu textuel.
    
    Args:
        file_source: Chemin vers le fichier PDF (str) ou objet fichier (UploadedFile, file-like object).
    
    Returns:
        str: Texte extrait du PDF.
    """
    try:
        reader = PdfReader(file_source)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text
    except Exception as e:
        return f"Erreur lors de la lecture du PDF : {str(e)}"

# Fonction pour ex√©cuter l'agent avec un texte de loi
@observe(name="run_agent_with_law_text")
def run_agent_with_law_text(law_text: str, user_request: str, max_chars: int = MAX_CHARS):
    """
    Ex√©cute l'agent LangGraph avec un texte de loi.
    L'agent d√©cide automatiquement quels outils utiliser.
    
    Args:
        law_text (str): Le texte de la loi √† analyser.
        user_request (str): La demande de l'utilisateur (r√©sum√©, analyse, etc.)
        max_chars (int): Nombre maximum de caract√®res √† traiter (d√©faut: 5000)
    
    Returns:
        str: R√©ponse finale de l'agent format√©e en Markdown.
    """
    
    # Limiter le texte pour √©viter d√©passement contexte et timeouts
    law_text_truncated = law_text[:max_chars]
    
    print(f"\nü§ñ L'agent analyse votre demande ({len(law_text_truncated)} caract√®res)...\n")
    
    # Construire la requ√™te compl√®te avec le texte de loi
    full_query = f"{user_request}\n\nTexte de la loi :\n{law_text_truncated}"
    
    initial_state = {
        "messages": [SYSTEM_PROMPT_SIMPLE_AGENT, HumanMessage(content=full_query)]
    }
    
    # Ex√©cuter le graph avec Langfuse tracing
    result = graph.invoke(
        initial_state,
        config={"callbacks": [langfuse_handler]}
    )
    
    # Extraire les r√©sultats des tools uniquement
    tool_results = []
    for message in result["messages"]:
        if type(message).__name__ == "ToolMessage":
            tool_results.append(f"## {message.name}\n\n{message.content}\n\n---\n")
    
    return "\n".join(tool_results) if tool_results else "Aucune r√©ponse g√©n√©r√©e."